import torch
import torch.nn as nn
from typing import Optional, Tuple, Dict, Any
import onnxscript
from diffusers.models.attention_processor import Attention
from QEfficient.customop.mmdit_attn_processor import JointAttnProcessor2_0Func, JointAttnProcessor2_0Onnx

CUSTOM_OPSET = onnxscript.values.Opset(domain="com.qualcomm.cloud", version=1)
# Import the ONNX Script opset for version 13
ops = getattr(onnxscript, "opset" + str(13))

@onnxscript.script(CUSTOM_OPSET)
def AttentionOnnx(
    hidden_states:  onnxscript.FLOAT,
    encoder_hidden_states_to_pass: onnxscript.FLOAT,
    attention_mask_to_pass: onnxscript.FLOAT,
    attn_heads: int,
    attn_head_dim: int,
    attn_scale: float,
    attn_query_dim: int,
    attn_inner_dim: int,
    attn_inner_kv_dim: int,
    to_q_weight: onnxscript.FLOAT,
    to_q_bias: onnxscript.FLOAT,
    to_k_weight: onnxscript.FLOAT,
    to_k_bias: onnxscript.FLOAT,
    to_v_weight: onnxscript.FLOAT,
    to_v_bias: onnxscript.FLOAT,
    norm_q_weight: onnxscript.FLOAT,
    norm_q_eps: float,
    norm_k_weight: onnxscript.FLOAT,
    norm_k_eps: float,
    add_q_proj_weight: onnxscript.FLOAT,
    add_q_proj_bias: onnxscript.FLOAT,
    add_k_proj_weight: onnxscript.FLOAT,
    add_k_proj_bias: onnxscript.FLOAT,
    add_v_proj_weight: onnxscript.FLOAT,
    add_v_proj_bias: onnxscript.FLOAT,
    norm_added_q_weight: onnxscript.FLOAT,
    norm_added_q_eps: float,
    norm_added_k_weight: onnxscript.FLOAT,
    norm_added_k_eps: float,
    to_out_0_weight: onnxscript.FLOAT,
    to_out_0_bias: onnxscript.FLOAT,
    to_out_1_dropout_p: float,
    attn_added_kv_proj_dim: int,
    to_add_out_weight: onnxscript.FLOAT,
    to_add_out_bias: onnxscript.FLOAT,
    attn_upcast_attention: bool,
    attn_upcast_softmax: bool,
    _original_encoder_hidden_states_was_none: bool,
    _original_attention_mask_was_none: bool,
    _original_input_onnx_dtype_code: int,                
):
    return JointAttnProcessor2_0Onnx(
        hidden_states,
        encoder_hidden_states_to_pass, # Prepared dummy or actual
        attention_mask_to_pass,        # Prepared dummy or actual
        attn_heads,
        attn_head_dim,
        attn_scale,
        attn_query_dim,
        attn_inner_dim,
        attn_inner_kv_dim,
        to_q_weight,
        to_q_bias,
        to_k_weight,
        to_k_bias,
        to_v_weight,
        to_v_bias,    
        norm_q_weight,
        norm_q_eps,
        norm_k_weight,
        norm_k_eps,
        add_q_proj_weight,
        add_q_proj_bias,
        add_k_proj_weight,
        add_k_proj_bias,
        add_v_proj_weight,
        add_v_proj_bias,
        norm_added_q_weight,
        norm_added_q_eps,
        norm_added_k_weight,
        norm_added_k_eps,
        to_out_0_weight,
        to_out_0_bias,
        to_out_1_dropout_p,
        attn_added_kv_proj_dim,
        to_add_out_weight,
        to_add_out_bias,
        attn_upcast_attention,
        attn_upcast_softmax,
        _original_encoder_hidden_states_was_none,
        _original_attention_mask_was_none,
        _original_input_onnx_dtype_code,                
    )
    
# This class will house the autograd.Function for the Attention block
class AttentionFunc(torch.autograd.Function):
    @staticmethod
    def forward(
        hidden_states: torch.Tensor,
        encoder_hidden_states_to_pass: torch.Tensor, # Prepared dummy or actual
        attention_mask_to_pass: torch.Tensor,        # Prepared dummy or actual
        # All parameters from JointAttnProcessor2_0Func (assuming 'self' from Attention)
        attn_heads: int,
        attn_head_dim: int,
        attn_scale: float,
        attn_query_dim: int,
        attn_inner_dim: int,
        attn_inner_kv_dim: int,
        to_q_weight: torch.Tensor,
        to_q_bias: torch.Tensor,
        to_k_weight: torch.Tensor,
        to_k_bias: torch.Tensor,
        to_v_weight: torch.Tensor,
        to_v_bias: torch.Tensor,
        norm_q_weight: torch.Tensor,
        norm_q_eps: float,
        norm_k_weight: torch.Tensor,
        norm_k_eps: float,
        add_q_proj_weight: torch.Tensor,
        add_q_proj_bias: torch.Tensor,
        add_k_proj_weight: torch.Tensor,
        add_k_proj_bias: torch.Tensor,
        add_v_proj_weight: torch.Tensor,
        add_v_proj_bias: torch.Tensor,
        norm_added_q_weight: torch.Tensor,
        norm_added_q_eps: float,
        norm_added_k_weight: torch.Tensor,
        norm_added_k_eps: float,
        to_out_0_weight: torch.Tensor,
        to_out_0_bias: torch.Tensor,
        to_out_1_dropout_p: float,
        attn_added_kv_proj_dim: int,
        to_add_out_weight: torch.Tensor,
        to_add_out_bias: torch.Tensor,
        attn_upcast_attention: bool,
        attn_upcast_softmax: bool,
        _original_encoder_hidden_states_was_none: bool,
        _original_attention_mask_was_none: bool,
        _original_input_onnx_dtype_code: int,
    ) -> torch.Tensor:

        # Call the lower-level processor's apply method
        # This calls the JointAttnProcessor2_0Func's forward method
        # Ensure the arguments exactly match its signature
        attn_output, context_attn_output = JointAttnProcessor2_0Func.apply(
            hidden_states,
            encoder_hidden_states_to_pass,
            attention_mask_to_pass,
            attn_heads,
            attn_head_dim,
            attn_scale,
            attn_query_dim,
            attn_inner_dim,
            attn_inner_kv_dim,
            to_q_weight,
            to_q_bias,
            to_k_weight,
            to_k_bias,
            to_v_weight,
            to_v_bias,
            norm_q_weight,
            norm_q_eps,
            norm_k_weight,
            norm_k_eps,
            add_q_proj_weight,
            add_q_proj_bias,
            add_k_proj_weight,
            add_k_proj_bias,
            add_v_proj_weight,
            add_v_proj_bias,
            norm_added_q_weight,
            norm_added_q_eps,
            norm_added_k_weight,
            norm_added_k_eps,
            to_out_0_weight,
            to_out_0_bias,
            to_out_1_dropout_p,
            attn_added_kv_proj_dim,
            to_add_out_weight,
            to_add_out_bias,
            attn_upcast_attention,
            attn_upcast_softmax,
            _original_encoder_hidden_states_was_none,
            _original_attention_mask_was_none,
            _original_input_onnx_dtype_code,
        )
        return attn_output

    @staticmethod
    def symbolic(
        g: torch.Graph,
        hidden_states: torch.Value,
        encoder_hidden_states_to_pass: torch.Value,
        attention_mask_to_pass: torch.Value,
        attn_heads: int,
        attn_head_dim: int,
        attn_scale: float,
        attn_query_dim: int,
        attn_inner_dim: int,
        attn_inner_kv_dim: int,
        to_q_weight: torch.Value,
        to_q_bias: torch.Value,
        to_k_weight: torch.Value,
        to_k_bias: torch.Value,
        to_v_weight: torch.Value,
        to_v_bias: torch.Value,
        norm_q_weight: torch.Value,
        norm_q_eps: torch.Value, # Changed from float to torch.Value for ONNX export
        norm_k_weight: torch.Value,
        norm_k_eps: torch.Value, # Changed from float to torch.Value for ONNX export
        add_q_proj_weight: torch.Value,
        add_q_proj_bias: torch.Value,
        add_k_proj_weight: torch.Value,
        add_k_proj_bias: torch.Value,
        add_v_proj_weight: torch.Value,
        add_v_proj_bias: torch.Value,
        norm_added_q_weight: torch.Value,
        norm_added_q_eps: torch.Value, # Changed from float to torch.Value for ONNX export
        norm_added_k_weight: torch.Value,
        norm_added_k_eps: torch.Value, # Changed from float to torch.Value for ONNX export
        to_out_0_weight: torch.Value,
        to_out_0_bias: torch.Value,
        to_out_1_dropout_p: torch.Value, # Changed from float to torch.Value for ONNX export
        attn_added_kv_proj_dim: int,
        to_add_out_weight: torch.Value,
        to_add_out_bias: torch.Value,
        attn_upcast_attention: bool,
        attn_upcast_softmax: bool,
        _original_encoder_hidden_states_was_none: bool,
        _original_attention_mask_was_none: bool,
        _original_input_onnx_dtype_code: int,
        # Add elementwise_affine parameters here if they are part of your real signature
    ) -> torch.Value:
        # Here we call the symbolic method of the underlying processor (JointAttnProcessor2_0Func)
        # This will construct the ONNX graph for the attention operation.
        attn_output, context_attn_output = g.onnxscript_op(AttentionOnnx,
            hidden_states,
            encoder_hidden_states_to_pass,
            attention_mask_to_pass,
            attn_heads,
            attn_head_dim,
            attn_scale,
            attn_query_dim,
            attn_inner_dim,
            attn_inner_kv_dim,
            to_q_weight,
            to_q_bias,
            to_k_weight,
            to_k_bias,
            to_v_weight,
            to_v_bias,
            norm_q_weight,
            norm_q_eps, # Pass the torch.Value here
            norm_k_weight,
            norm_k_eps, # Pass the torch.Value here
            add_q_proj_weight,
            add_q_proj_bias,
            add_k_proj_weight,
            add_k_proj_bias,
            add_v_proj_weight,
            add_v_proj_bias,
            norm_added_q_weight,
            norm_added_q_eps, # Pass the torch.Value here
            norm_added_k_weight,
            norm_added_k_eps, # Pass the torch.Value here
            to_out_0_weight,
            to_out_0_bias,
            to_out_1_dropout_p, # Pass the torch.Value here
            attn_added_kv_proj_dim,
            to_add_out_weight,
            to_add_out_bias,
            attn_upcast_attention,
            attn_upcast_softmax,
            _original_encoder_hidden_states_was_none,
            _original_attention_mask_was_none,
            _original_input_onnx_dtype_code,
        )
        return attn_output
    

class AttentionAIC(nn.Module):
    """
    Dummy AttentionAIC module that just extracts parameters and calls AttentionFuncAIC.apply.
    This replaces the original Attention module in the model.
    """
    def __init__(self, original_module: Attention):
        super().__init__()
    
    def forward(
        self,
        hidden_states: torch.FloatTensor,
        encoder_hidden_states: Optional[torch.FloatTensor],
        attention_mask: Optional[torch.FloatTensor],
        attn_heads:int,
        attn_head_dim: int,
        attn_scale: float,
        attn_query_dim: int,
        attn_inner_dim: int,
        attn_inner_kv_dim: int,
        to_q_weight: torch.Tensor,
        to_q_bias: torch.Tensor,
        to_k_weight: torch.Tensor,
        to_k_bias: torch.Tensor,
        to_v_weight: torch.Tensor,
        to_v_bias: torch.Tensor,
        norm_q_weight: torch.Tensor,
        norm_q_eps: float,
        norm_k_weight: torch.Tensor,
        norm_k_eps: float,
        add_q_proj_weight: torch.Tensor,
        add_q_proj_bias: torch.Tensor,
        add_k_proj_weight: torch.Tensor,
        add_k_proj_bias: torch.Tensor,
        add_v_proj_weight: torch.Tensor,
        add_v_proj_bias: torch.Tensor,
        norm_added_q_weight: torch.Tensor,
        norm_added_q_eps: float,
        norm_added_k_weight: torch.Tensor,
        norm_added_k_eps: float,
        to_out_0_weight: torch.Tensor,
        to_out_0_bias: torch.Tensor,
        to_out_1_dropout_p: float,
        attn_added_kv_proj_dim: int,
        to_add_out_weight: torch.Tensor,
        to_add_out_bias: torch.Tensor,
        attn_upcast_attention: bool,
        attn_upcast_softmax: bool,
        _original_encoder_hidden_states_was_none: bool,
        _original_attention_mask_was_none: bool,
        _original_input_onnx_dtype_code: int,
    ) -> Tuple[torch.FloatTensor, torch.FloatTensor]:

        # Convert float parameters to torch.Tensor for the autograd.Function call
        # norm_q_eps_tensor = torch.tensor(self.norm_q_eps, dtype=dtype, device=device)
        # norm_k_eps_tensor = torch.tensor(self.norm_k_eps, dtype=dtype, device=device)
        # norm_added_q_eps_tensor = torch.tensor(self.norm_added_q_eps, dtype=dtype, device=device)
        # norm_added_k_eps_tensor = torch.tensor(self.norm_added_k_eps, dtype=dtype, device=device)
        # to_out_1_dropout_p_tensor = torch.tensor(self.to_out_1_dropout_p, dtype=dtype, device=device)

        # Call the higher-level AttentionFuncAIC.apply
        return AttentionFunc.apply(
            hidden_states,
            encoder_hidden_states,
            attention_mask,
            attn_heads,
            attn_head_dim,
            attn_scale,
            attn_query_dim,
            attn_inner_dim,
            attn_inner_kv_dim,
            to_q_weight,
            to_q_bias,
            to_k_weight,
            to_k_bias,
            to_v_weight,
            to_v_bias,
            norm_q_weight,
            norm_q_eps,
            norm_k_weight,
            norm_k_eps,
            add_q_proj_weight,
            add_q_proj_bias,
            add_k_proj_weight,
            add_k_proj_bias,
            add_v_proj_weight,
            add_v_proj_bias,
            norm_added_q_weight,
            norm_added_q_eps,
            norm_added_k_weight,
            norm_added_k_eps,
            to_out_0_weight,
            to_out_0_bias,
            to_out_1_dropout_p,
            attn_added_kv_proj_dim,
            to_add_out_weight,
            to_add_out_bias,
            attn_upcast_attention,
            attn_upcast_softmax,
            _original_encoder_hidden_states_was_none,
            _original_attention_mask_was_none,
            _original_input_onnx_dtype_code,
        )